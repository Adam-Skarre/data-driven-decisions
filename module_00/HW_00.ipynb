{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c0b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ryan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382f8f1",
   "metadata": {},
   "source": [
    "# HW_00 - Generative AI writing analysis\n",
    "\n",
    "In this assignment, you will generate instructions on brushing your teeth. \n",
    "\n",
    "Some metrics to add to the generated instructions:\n",
    "\n",
    "- number of strokes per minute\n",
    "- total brushing time\n",
    "- time spent per tooth \n",
    "- number of teeth or total area to brush on teeth\n",
    "- deflection of brush bristles for proper cleaning\n",
    "- what else can you think of?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70900f",
   "metadata": {},
   "source": [
    "## Prompt Input and Output\n",
    "\n",
    "-> _copy-paste your prompts and outputs here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2138771",
   "metadata": {},
   "source": [
    "## Revised document\n",
    "\n",
    "-> _copy-paste the document here, then edit the output to remove passive phrasing and add specific ideas from your own research or experience (try quantifying any phrases such as 'many', 'fewer', 'more important', etc._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659840ba",
   "metadata": {},
   "source": [
    "## Document analysis\n",
    "\n",
    "- Make a list of all the improvements and changes you made to document\n",
    "- use the `tf_idf.cosineSimilarity` function to compare the AI version to your own\n",
    "\n",
    "Write a report on your intellectual property  in the 'revised document'. \n",
    "- How much can you claim as yours?\n",
    "- How many ideas came from AI?\n",
    "- How many ideas came from you?\n",
    "- Is this a _new_ document?\n",
    "- If this work was made by you and another person-not AI-would you need to credit this person as a coauthor?\n",
    "- What else can you discuss about this comparison and this process?\n",
    "\n",
    "_run the cell below to get your `tf_idf` functions ready to run_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51928f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import sys\n",
    "\n",
    "# https://medium.com/@mifthulyn07/comparing-text-documents-using-tf-idf-and-cosine-similarity-in-python-311863c74b2c\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # lowercasing\n",
    "    lowercased_text = text.lower()\n",
    "\n",
    "    # cleaning \n",
    "    import re \n",
    "    remove_punctuation = re.sub(r'[^\\w\\s]', '', lowercased_text)\n",
    "    remove_white_space = remove_punctuation.strip()\n",
    "\n",
    "    # Tokenization = Breaking down each sentence into an array\n",
    "    # from nltk.tokenize import word_tokenize\n",
    "    tokenized_text = word_tokenize(remove_white_space)\n",
    "\n",
    "    # Stop Words/filtering = Removing irrelevant words\n",
    "    # from nltk.corpus import stopwords\n",
    "    # stopwords = set(stopwords.words('english'))\n",
    "    stopwords_removed = [word for word in tokenized_text if word not in stopwords.words()]\n",
    "\n",
    "    # Stemming = Transforming words into their base form\n",
    "    #from nltk.stem import PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = [ps.stem(word) for word in stopwords_removed]\n",
    "    \n",
    "    # Putting all the results into a dataframe.\n",
    "    df = pd.DataFrame({\n",
    "        'DOCUMENT': [text],\n",
    "        'LOWERCASE' : [lowercased_text],\n",
    "        'CLEANING': [remove_white_space],\n",
    "        'TOKENIZATION': [tokenized_text],\n",
    "        'STOP-WORDS': [stopwords_removed],\n",
    "        'STEMMING': [stemmed_text]\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_tfidf(df):\n",
    "    # Call the preprocessing result\n",
    "    #df = preprocessing(corpus)\n",
    "        \n",
    "    # Make each array row from stopwords_removed to be a sentence\n",
    "    stemming = df['STEMMING'].apply(' '.join)\n",
    "    \n",
    "    # Count TF-IDF\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(stemming)\n",
    "    \n",
    "    # Get words from stopwords array to use as headers\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Combine header titles and weights\n",
    "    df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "    df_tfidf = pd.concat([df, df_tfidf], axis=1)\n",
    "\n",
    "    return df_tfidf\n",
    "\n",
    "\n",
    "\n",
    "def cosineSimilarity(df):\n",
    "    # Call the TF-IDF result\n",
    "    df_tfidf = calculate_tfidf(df)\n",
    "    \n",
    "    # Get the TF-IDF vector for the first item (index 0)\n",
    "    vector1 = df_tfidf.iloc[0, 6:].values.reshape(1, -1)\n",
    "\n",
    "    # Get the TF-IDF vector for all items except the first item\n",
    "    vectors = df_tfidf.iloc[:, 6:].values\n",
    "    \n",
    "    # Calculate cosine similarity between the first item and all other items\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    cosim = cosine_similarity(vector1, vectors)\n",
    "    cosim = pd.DataFrame(cosim)\n",
    "    \n",
    "    # Convert the DataFrame into a one-dimensional array\n",
    "    cosim = cosim.values.flatten()\n",
    "\n",
    "    # Convert the cosine similarity result into a DataFrame\n",
    "    df_cosim = pd.DataFrame(cosim, columns=['COSIM'])\n",
    "\n",
    "    # Combine the TF-IDF array with the cosine similarity result\n",
    "    df_cosim = pd.concat([df_tfidf, df_cosim], axis=1)\n",
    "\n",
    "    return df_cosim[['DOCUMENT', 'STEMMING', 'COSIM']]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
